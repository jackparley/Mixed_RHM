{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e566a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import *\n",
    "import warnings\n",
    "import copy\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#from .utils import dec2bin, dec2base, base2dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633932df",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 16\n",
    "f=1/4\n",
    "m_2=int(f*v)\n",
    "m_3=int(f*v**2)\n",
    "s_2=2\n",
    "s_3=3\n",
    "L=2\n",
    "seed_rules=0\n",
    "n=v\n",
    "fac=1000\n",
    "num_features = v\n",
    "h=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a1ac4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mixed_rules(v, n, m_2, m_3, s_2, s_3, L, seed):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Define a callable for the inner defaultdict to return empty tensors\n",
    "    def tensor_default():\n",
    "        return torch.empty(0)\n",
    "\n",
    "    tuples_2 = list(product(*[range(v) for _ in range(s_2)]))\n",
    "    tuples_3 = list(product(*[range(v) for _ in range(s_3)]))\n",
    "\n",
    "    # Define the nested defaultdict structure\n",
    "    rules = defaultdict(lambda: defaultdict(tensor_default))\n",
    "\n",
    "    # Initialize the grammar with sampled tensors\n",
    "    rules[0][0] = torch.tensor(random.sample(tuples_2, n * m_2)).reshape(n, m_2, s_2)\n",
    "    rules[0][1] = torch.tensor(random.sample(tuples_3, n * m_3)).reshape(n, m_3, s_3)\n",
    "    for i in range(1, L):\n",
    "        rules[i][0] = torch.tensor(random.sample(tuples_2, v * m_2)).reshape(\n",
    "            v, m_2, s_2\n",
    "        )\n",
    "        rules[i][1] = torch.tensor(random.sample(tuples_3, v * m_3)).reshape(\n",
    "            v, m_3, s_3\n",
    "        )\n",
    "\n",
    "    return rules\n",
    "\n",
    "def index_to_choice_d_5(index, n,m2,m3):\n",
    "    Pmax=n*m2*m3*m2\n",
    "    if index <Pmax:\n",
    "\n",
    "        bases = [n, m2,m3,m2]  # Alternating base sizes\n",
    "        #index -= 1  # Convert to 0-based index\n",
    "        choice = []\n",
    "\n",
    "        # Compute the total number of possibilities\n",
    "        total_combinations = 1\n",
    "        for base in bases:\n",
    "            total_combinations *= base\n",
    "\n",
    "        # Extract choices one by one\n",
    "        for base in bases:\n",
    "            total_combinations //= base  # Reduce divisor dynamically\n",
    "            choice.append(index // total_combinations + 1)\n",
    "            index %= total_combinations  # Reduce index to the remainder\n",
    "    else:\n",
    "        index = index-Pmax\n",
    "        bases = [n, m2,m2,m3]\n",
    "        choice = []\n",
    "\n",
    "        # Compute the total number of possibilities\n",
    "        total_combinations = 1\n",
    "        for base in bases:\n",
    "            total_combinations *= base\n",
    "\n",
    "        # Extract choices one by one\n",
    "        for base in bases:\n",
    "            total_combinations //= base  # Reduce divisor dynamically\n",
    "            choice.append(index // total_combinations + 1)\n",
    "            index %= total_combinations  # Reduce index to the remainder\n",
    "    return choice\n",
    "\n",
    "def sample_data_from_indices_d_5(\n",
    "    samples, rules, n, m_2, m_3\n",
    "):\n",
    "    L = len(rules)\n",
    "    Pmax=n*m_2*m_3*m_2\n",
    "    all_features = []\n",
    "    labels = []\n",
    "    #samples = samples + 1\n",
    "    for sample in samples:\n",
    "        chosen_rules = index_to_choice_d_5(sample, n, m_2, m_3)\n",
    "        labels.append(chosen_rules[0] - 1)\n",
    "        # print(chosen_rules[0])\n",
    "        chosen_rules = [x - 1 for x in chosen_rules]\n",
    "        # for label in labels:\n",
    "        # Initialize the current symbols with the start symbol\n",
    "        current_symbols = [chosen_rules[0]]\n",
    "\n",
    "        # Sequentially apply rules for each layer\n",
    "        k = 0\n",
    "        k_2 = 1\n",
    "        if sample <Pmax:\n",
    "            rule_types = [0, 1, 0]\n",
    "        else:\n",
    "            rule_types = [0, 0, 1]\n",
    "        \n",
    "        for layer in range(0, L):  # 1 to 3 (3 layers)\n",
    "            new_symbols = []\n",
    "            for symbol in current_symbols:\n",
    "                rule_type = rule_types[k]\n",
    "                k = k + 1\n",
    "                # print(rule_type)\n",
    "                rule_tensor = rules[layer][rule_type]\n",
    "                # chosen_rule=torch.randint(low=0,high=rule_tensor.shape[1],size=(1,)).item()\n",
    "                chosen_rule = chosen_rules[k_2]\n",
    "                k_2 = k_2 + 1\n",
    "                new_symbols.extend(rule_tensor[symbol, chosen_rule].tolist())\n",
    "            # print(new_symbols)\n",
    "            # new_symbols=new_symbols[0]\n",
    "            # print(new_symbols)\n",
    "            if new_symbols != []:\n",
    "                current_symbols = new_symbols\n",
    "            features = torch.tensor(new_symbols)\n",
    "        all_features.append(features)\n",
    "    concatenated_features = torch.cat(all_features).reshape(len(labels), -1)\n",
    "    labels = torch.tensor(labels)\n",
    "    return concatenated_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cdefd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplets_to_num(xi, n):\n",
    "    \"\"\"\n",
    "    Convert one long input with n-features encoding to n^3 triplet encoding.\n",
    "    \"\"\"\n",
    "    ln = len(xi)\n",
    "    xin = torch.zeros(ln // 3)\n",
    "    for ii, xii in enumerate(xi):\n",
    "        xin[ii // 3] += xii * n ** (2 - ii % 3)\n",
    "    return xin\n",
    "\n",
    "\n",
    "def tripling_features(x, n):\n",
    "    \"\"\"\n",
    "    Batch of inputs from n to n^3 encoding using triplets.\n",
    "    \"\"\"\n",
    "    xn = torch.zeros(x.shape[0], x.shape[-1] // 3)\n",
    "    for i, xi in enumerate(x.squeeze()):\n",
    "        xn[i] = triplets_to_num(xi, n)\n",
    "    return xn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_base_v(xi, v):\n",
    "    \"\"\"\n",
    "    Encode a list of digits in base-v to a single integer.\n",
    "    Most significant digit first.\n",
    "    \"\"\"\n",
    "    encoded = 0\n",
    "    for i, digit in enumerate(reversed(xi)):\n",
    "        encoded += digit * (v ** i)\n",
    "    return encoded\n",
    "\n",
    "def triplet_pairing_features_T1(x, v):\n",
    "    \"\"\"\n",
    "    Convert inputs of shape [N, 5] into [N, 2] where:\n",
    "        - output[:, 0] encodes x[:, :3] as base-v triplet -> int in [0, v^3-1]\n",
    "        - output[:, 1] encodes x[:, 3:5] as base-v pair   -> int in [0, v^2-1]\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x_out = torch.zeros(N, 2, dtype=torch.long)\n",
    "\n",
    "    for i in range(N):\n",
    "        triplet = x[i, :3]\n",
    "        pair = x[i, 3:5]\n",
    "        x_out[i, 0] = encode_base_v(triplet, v)\n",
    "        x_out[i, 1] = encode_base_v(pair, v)\n",
    "\n",
    "    return x_out\n",
    "\n",
    "\n",
    "def triplet_pairing_features_T2(x, v):\n",
    "    \"\"\"\n",
    "    Convert inputs of shape [N, 5] into [N, 2] where:\n",
    "        - output[:, 0] encodes x[:, :3] as base-v triplet -> int in [0, v^3-1]\n",
    "        - output[:, 1] encodes x[:, 3:5] as base-v pair   -> int in [0, v^2-1]\n",
    "    \"\"\"\n",
    "    N = x.shape[0]\n",
    "    x_out = torch.zeros(N, 2, dtype=torch.long)\n",
    "\n",
    "    for i in range(N):\n",
    "        triplet = x[i, 2:5]\n",
    "        pair = x[i, :2]\n",
    "        x_out[i, 0] = encode_base_v(pair, v)\n",
    "        x_out[i, 1] = encode_base_v(triplet, v)\n",
    "\n",
    "    return x_out\n",
    "\n",
    "\n",
    "\n",
    "def pairs_to_num(xi, n):\n",
    "\n",
    "    \"\"\"\n",
    "        Convert one long input with n-features encoding to n^2 pairs encoding.\n",
    "    \"\"\"\n",
    "    ln = len(xi)\n",
    "    xin = torch.zeros(ln // 2)\n",
    "    for ii, xii in enumerate(xi):\n",
    "        xin[ii // 2] += xii * n ** (1 - ii % 2)\n",
    "    return xin\n",
    "\n",
    "def pairing_features(x, n):\n",
    "    \"\"\"\n",
    "        Batch of inputs from n to n^2 encoding.\n",
    "    \"\"\"\n",
    "    xn = torch.zeros(x.shape[0], x.shape[-1] // 2)\n",
    "    for i, xi in enumerate(x.squeeze()):\n",
    "        xn[i] = pairs_to_num(xi, n)\n",
    "    return xn\n",
    "\n",
    "def two_layers(w1, seed, x, y):\n",
    "    h, v2 = w1.size()\n",
    "    assert v2 == x.shape[-1], \"Input dim. not matching!\"\n",
    "    v = int(v2 ** .5)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    w2 = torch.randn(v, h, generator=g)\n",
    "\n",
    "    o = (w2 @ (w1 @ x.t()).div(v2 ** .5).relu() / h).t()\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(o, y, reduction=\"mean\")\n",
    "\n",
    "    return loss, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76be25fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_T1(features, labels, mask,fac,h,seed_net):\n",
    "    x = features[mask]\n",
    "    y = labels[mask]\n",
    "    x_pair = triplet_pairing_features_T1(x, v)\n",
    "    x0 = F.one_hot(x_pair[:, 0], num_classes=v ** 3).float()\n",
    "    x1 = F.one_hot(x_pair[:, 1], num_classes=v ** 2).float()\n",
    "    \n",
    "    w0 = torch.ones(h, v ** 3, requires_grad=True)\n",
    "    loss0, _ = two_layers(w0, seed_net, x0, y)\n",
    "    loss0.backward()\n",
    "    grad0 = w0.grad.clone()\n",
    "\n",
    "    w1 = torch.ones(h, v ** 2, requires_grad=True)\n",
    "    loss1, _ = two_layers(w1, seed_net, x1, y)\n",
    "    loss1.backward()\n",
    "    grad1 = w1.grad.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ws0 = nn.Parameter(w0 - grad0)\n",
    "        ws1 = nn.Parameter(w1 - grad1)\n",
    "\n",
    "    kms0 = KMeans(n_clusters=v, n_init=10).fit(ws0.t().detach().numpy())\n",
    "    kms1 = KMeans(n_clusters=v, n_init=10).fit(ws1.t().detach().numpy())\n",
    "    \n",
    "    labels_0 = torch.tensor(kms0.labels_)[triplet_pairing_features_T1(features, v).int()[:, 0]]\n",
    "    labels_1 = torch.tensor(kms1.labels_)[triplet_pairing_features_T1(features, v).int()[:, 1]]\n",
    "    combined = torch.stack([labels_0, labels_1], dim=1)\n",
    "    \n",
    "    x = pairing_features(combined.numpy(), v)\n",
    "    x = F.one_hot(x.long(), num_classes=v ** 2).float().permute(0, 2, 1)\n",
    "    \n",
    "    w_top = nn.Parameter(torch.ones(h, v ** 2))\n",
    "    loss, o = two_layers(w_top, seed_net + 1, x[..., 0], labels)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_top -= fac * h * w_top.grad\n",
    "        loss, o = two_layers(w_top, seed_net + 1, x[..., 0], labels)\n",
    "    \n",
    "    per_datum_loss = F.cross_entropy(o, labels, reduction=\"none\")\n",
    "    return per_datum_loss,w_top,kms0,kms1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2b7c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_T2(features, labels, mask,fac,h,seed_net):\n",
    "    x = features[mask]\n",
    "    y = labels[mask]\n",
    "    x_pair = triplet_pairing_features_T2(x, v)\n",
    "    x0 = F.one_hot(x_pair[:, 0], num_classes=v ** 2).float()\n",
    "    x1 = F.one_hot(x_pair[:, 1], num_classes=v ** 3).float()\n",
    "    \n",
    "    w0 = torch.ones(h, v ** 2, requires_grad=True)\n",
    "    loss0, _ = two_layers(w0, seed_net, x0, y)\n",
    "    loss0.backward()\n",
    "    grad0 = w0.grad.clone()\n",
    "\n",
    "    w1 = torch.ones(h, v ** 3, requires_grad=True)\n",
    "    loss1, _ = two_layers(w1, seed_net, x1, y)\n",
    "    loss1.backward()\n",
    "    grad1 = w1.grad.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ws0 = nn.Parameter(w0 - grad0)\n",
    "        ws1 = nn.Parameter(w1 - grad1)\n",
    "\n",
    "    kms0 = KMeans(n_clusters=v, n_init=10).fit(ws0.t().detach().numpy())\n",
    "    kms1 = KMeans(n_clusters=v, n_init=10).fit(ws1.t().detach().numpy())\n",
    "    \n",
    "    labels_0 = torch.tensor(kms0.labels_)[triplet_pairing_features_T2(features, v).int()[:, 0]]\n",
    "    labels_1 = torch.tensor(kms1.labels_)[triplet_pairing_features_T2(features, v).int()[:, 1]]\n",
    "    combined = torch.stack([labels_0, labels_1], dim=1)\n",
    "    \n",
    "    x = pairing_features(combined.numpy(), v)\n",
    "    x = F.one_hot(x.long(), num_classes=v ** 2).float().permute(0, 2, 1)\n",
    "    \n",
    "    w_top = nn.Parameter(torch.ones(h, v ** 2))\n",
    "    loss, o = two_layers(w_top, seed_net + 1, x[..., 0], labels)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w_top -= fac * h * w_top.grad\n",
    "        loss, o = two_layers(w_top, seed_net + 1, x[..., 0], labels)\n",
    "    \n",
    "    per_datum_loss = F.cross_entropy(o, labels, reduction=\"none\")\n",
    "    return per_datum_loss,w_top,kms0,kms1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba97729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combined_model(features_test, labels_test, \n",
    "                            kms0_T1, kms1_T1, w_top_T1,\n",
    "                            kms0_T2, kms1_T2, w_top_T2,\n",
    "                            seed_net, v, tree_origin):\n",
    "    l = 1\n",
    "    y = labels_test\n",
    "    \n",
    "    def get_logits(features_test, kms0, kms1, ws_top, triplet_fn):\n",
    "        labels_0 = torch.tensor(kms0.labels_)[triplet_fn(features_test, v).int()[:, 0]]\n",
    "        labels_1 = torch.tensor(kms1.labels_)[triplet_fn(features_test, v).int()[:, 1]]\n",
    "        combined = torch.stack([labels_0, labels_1], dim=1)\n",
    "        x = pairing_features(combined.numpy(), v)\n",
    "        x = F.one_hot(x.long(), num_classes=v ** 2).float().permute(0, 2, 1).squeeze(-1)\n",
    "        with torch.no_grad():\n",
    "            _, logits = two_layers(ws_top, seed_net + l, x, y)\n",
    "        return logits\n",
    "\n",
    "    # Get logits for both models\n",
    "    logits_T1 = get_logits(features_test, kms0_T1, kms1_T1, w_top_T1, triplet_pairing_features_T1)\n",
    "    logits_T2 = get_logits(features_test, kms0_T2, kms1_T2, w_top_T2, triplet_pairing_features_T2)\n",
    "\n",
    "    # Softmax + confidence\n",
    "    probs_T1 = F.softmax(logits_T1, dim=1)\n",
    "    probs_T2 = F.softmax(logits_T2, dim=1)\n",
    "    conf_T1, preds_T1 = torch.max(probs_T1, dim=1)\n",
    "    conf_T2, preds_T2 = torch.max(probs_T2, dim=1)\n",
    "\n",
    "    # Choose most confident prediction\n",
    "    use_T1 = conf_T1 > conf_T2\n",
    "    final_preds = torch.where(use_T1, preds_T1, preds_T2)\n",
    "\n",
    "    # Compute test error\n",
    "    test_error = (final_preds != y).float().mean().item()\n",
    "    # 0 = T1, 1 = T2\n",
    "    predicted_tree = (~use_T1).long()  # use_T1==True → predict T1 (0), otherwise T2 (1)\n",
    "    tree_error = (predicted_tree != tree_origin).float().mean().item()\n",
    "    return test_error,tree_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "953689b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= Running for P = 30000 =========\n",
      "  -- Realization 1 --\n",
      "    Vanilla test error: 0.7170, Vanilla tree error: 0.4580\n",
      "Best test error: 0.2535, Best tree error: 0.1440, Best iter: 6, Best tree iter: 6, Best assingment error: 0.0321, Single step test error: 0.3420, Single step tree error: 0.2040, Single step assingment errror: 0.1011\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import torch\n",
    "\n",
    "#PP = np.logspace(np.log10(100), np.log10(30000), num=8, dtype=int)\n",
    "PP=[30000]\n",
    "results = defaultdict(list)\n",
    "num_realizations = 1\n",
    "\n",
    "for P in PP:\n",
    "    print(f\"\\n========= Running for P = {P} =========\")\n",
    "\n",
    "    vanilla_errors = []\n",
    "    vanilla_tree_errors = []\n",
    "    single_step_errors = []\n",
    "    single_step_tree_errors = []\n",
    "    final_errors = []\n",
    "    final_tree_errors = []\n",
    "    best_errors = []\n",
    "    best_tree_errors = []\n",
    "    best_error_iters = []\n",
    "    best_tree_error_iters = []\n",
    "    num_iters = []\n",
    "    best_incorrect_fracs = []\n",
    "    single_step_incorrect_fracs = []\n",
    "\n",
    "    for realization in range(num_realizations):\n",
    "        print(f\"  -- Realization {realization + 1} --\")\n",
    "\n",
    "        max_data = v * m_2**2 * m_3\n",
    "        train_size = P\n",
    "        test_size = 2000\n",
    "\n",
    "        full_range = list(range(2 * max_data))\n",
    "        samples = random.sample(full_range, train_size)\n",
    "        remaining_pool = list(set(full_range) - set(samples))\n",
    "        samples_test = random.sample(remaining_pool, test_size)\n",
    "\n",
    "        samples = torch.tensor(samples)\n",
    "        samples_test = torch.tensor(samples_test)\n",
    "        tree_origin = (samples_test >= max_data).long()\n",
    "        tree_origin_train = (samples >= max_data).long()\n",
    "\n",
    "        seed_rules = random.randint(0, 1000000)\n",
    "        rules = sample_mixed_rules(v, n, m_2, m_3, s_2, s_3, L, seed_rules)\n",
    "\n",
    "        features, labels = sample_data_from_indices_d_5(samples, rules, n, m_2, m_3)\n",
    "        features_test, labels_test = sample_data_from_indices_d_5(samples_test, rules, n, m_2, m_3)\n",
    "\n",
    "        N = len(features)\n",
    "        assign_T1 = torch.ones(N, dtype=torch.bool)\n",
    "        assign_T2 = torch.ones(N, dtype=torch.bool)\n",
    "\n",
    "        seed_net = random.randint(0, 1000000)\n",
    "        losses_T1, w_top_T1, kms0_T1, kms1_T1 = train_model_T1(features, labels, assign_T1, fac, h, seed_net)\n",
    "        losses_T2, w_top_T2, kms0_T2, kms1_T2 = train_model_T2(features, labels, assign_T2, fac, h, seed_net)\n",
    "\n",
    "        new_assign_T1 = losses_T1 < losses_T2\n",
    "        new_assign_T2 = ~new_assign_T1\n",
    "        assign_T1 = new_assign_T1\n",
    "        assign_T2 = new_assign_T2\n",
    "\n",
    "        vanilla_test_error, vanilla_tree_error = evaluate_combined_model(\n",
    "            features_test, labels_test,\n",
    "            kms0_T1, kms1_T1, w_top_T1,\n",
    "            kms0_T2, kms1_T2, w_top_T2,\n",
    "            seed_net, v, tree_origin\n",
    "        )\n",
    "        print(f\"    Vanilla test error: {vanilla_test_error:.4f}, Vanilla tree error: {vanilla_tree_error:.4f}\")\n",
    "\n",
    "        max_iters = 10\n",
    "        best_test_error = vanilla_test_error\n",
    "        best_tree_error = vanilla_tree_error\n",
    "        best_iter = 0\n",
    "        best_iter_tree = 0\n",
    "        final_test_error = None\n",
    "        final_tree_error = None\n",
    "        single_step_test_error = None\n",
    "        single_step_tree_error = None\n",
    "        best_incorrect_frac = None\n",
    "        single_step_incorrect_frac = None\n",
    "\n",
    "        for iteration in range(1, max_iters + 1):\n",
    "            losses_T1, w_top_T1, kms0_T1, kms1_T1 = train_model_T1(features, labels, assign_T1, fac, h, seed_net)\n",
    "            losses_T2, w_top_T2, kms0_T2, kms1_T2 = train_model_T2(features, labels, assign_T2, fac, h, seed_net)\n",
    "\n",
    "            new_assign_T1 = losses_T1 < losses_T2\n",
    "            new_assign_T2 = ~new_assign_T1\n",
    "\n",
    "            predicted_tree_train = (assign_T1).long()\n",
    "            incorrect_assign_frac = (predicted_tree_train == tree_origin_train).float().mean().item()\n",
    "\n",
    "            current_test_error, current_tree_error = evaluate_combined_model(\n",
    "                features_test, labels_test,\n",
    "                kms0_T1, kms1_T1, w_top_T1,\n",
    "                kms0_T2, kms1_T2, w_top_T2,\n",
    "                seed_net, v, tree_origin\n",
    "            )\n",
    "\n",
    "            if iteration == 1:\n",
    "                single_step_test_error = current_test_error\n",
    "                single_step_tree_error = current_tree_error\n",
    "                single_step_incorrect_frac = incorrect_assign_frac\n",
    "\n",
    "            if current_test_error < best_test_error:\n",
    "                best_test_error = current_test_error\n",
    "                best_iter = iteration\n",
    "                best_tree_error = current_tree_error\n",
    "                best_iter_tree = iteration\n",
    "                best_incorrect_frac = incorrect_assign_frac\n",
    "\n",
    "            if torch.equal(assign_T1, new_assign_T1):\n",
    "                final_test_error = current_test_error\n",
    "                final_tree_error = current_tree_error\n",
    "                break\n",
    "\n",
    "            assign_T1 = new_assign_T1\n",
    "            assign_T2 = new_assign_T2\n",
    "\n",
    "        if final_test_error is None:\n",
    "            final_test_error = current_test_error\n",
    "        if final_tree_error is None:\n",
    "            final_tree_error = current_tree_error\n",
    "\n",
    "        if best_incorrect_frac is None:\n",
    "            best_incorrect_frac = incorrect_assign_frac\n",
    "\n",
    "        print(f\"Best test error: {best_test_error:.4f}, Best tree error: {best_tree_error:.4f}, \"\n",
    "              f\"Best iter: {best_iter}, Best tree iter: {best_iter_tree}, \"\n",
    "              f\"Best assingment error: {best_incorrect_frac:.4f}, \"\n",
    "              f\"Single step test error: {single_step_test_error:.4f}, \"\n",
    "              f\"Single step tree error: {single_step_tree_error:.4f}, \"\n",
    "              f\"Single step assingment errror: {single_step_incorrect_frac:.4f}\")\n",
    "\n",
    "        vanilla_errors.append(vanilla_test_error)\n",
    "        vanilla_tree_errors.append(vanilla_tree_error)\n",
    "        single_step_errors.append(single_step_test_error)\n",
    "        single_step_tree_errors.append(single_step_tree_error)\n",
    "        final_errors.append(final_test_error)\n",
    "        final_tree_errors.append(final_tree_error)\n",
    "        best_errors.append(best_test_error)\n",
    "        best_tree_errors.append(best_tree_error)\n",
    "        best_error_iters.append(best_iter)\n",
    "        best_tree_error_iters.append(best_iter_tree)\n",
    "        num_iters.append(iteration)\n",
    "        best_incorrect_fracs.append(best_incorrect_frac)\n",
    "        single_step_incorrect_fracs.append(single_step_incorrect_frac)\n",
    "\n",
    "    results[\"P\"].append(P)\n",
    "\n",
    "    results[\"vanilla_error\"].append(np.mean(vanilla_errors))\n",
    "    results[\"vanilla_error_std\"].append(np.std(vanilla_errors))\n",
    "\n",
    "    results[\"vanilla_tree_error\"].append(np.mean(vanilla_tree_errors))\n",
    "    results[\"vanilla_tree_error_std\"].append(np.std(vanilla_tree_errors))\n",
    "\n",
    "    results[\"single_step_error\"].append(np.mean(single_step_errors))\n",
    "    results[\"single_step_error_std\"].append(np.std(single_step_errors))\n",
    "\n",
    "    results[\"single_step_tree_error\"].append(np.mean(single_step_tree_errors))\n",
    "    results[\"single_step_tree_error_std\"].append(np.std(single_step_tree_errors))\n",
    "\n",
    "    results[\"final_error\"].append(np.mean(final_errors))\n",
    "    results[\"final_error_std\"].append(np.std(final_errors))\n",
    "\n",
    "    results[\"final_tree_error\"].append(np.mean(final_tree_errors))\n",
    "    results[\"final_tree_error_std\"].append(np.std(final_tree_errors))\n",
    "\n",
    "    results[\"best_error\"].append(np.mean(best_errors))\n",
    "    results[\"best_error_std\"].append(np.std(best_errors))\n",
    "\n",
    "    results[\"best_tree_error\"].append(np.mean(best_tree_errors))\n",
    "    results[\"best_tree_error_std\"].append(np.std(best_tree_errors))\n",
    "\n",
    "    results[\"best_error_iter\"].append(np.mean(best_error_iters))\n",
    "    results[\"best_error_iter_std\"].append(np.std(best_error_iters))\n",
    "\n",
    "    results[\"best_tree_error_iter\"].append(np.mean(best_tree_error_iters))\n",
    "    results[\"best_tree_error_iter_std\"].append(np.std(best_tree_error_iters))\n",
    "\n",
    "    results[\"num_iterations\"].append(np.mean(num_iters))\n",
    "    results[\"num_iterations_std\"].append(np.std(num_iters))\n",
    "\n",
    "    results[\"best_assignment_error\"].append(np.mean(best_incorrect_fracs))\n",
    "    results[\"best_assignment_error_std\"].append(np.std(best_incorrect_fracs))\n",
    "\n",
    "    results[\"single_step_assignment_error\"].append(np.mean(single_step_incorrect_fracs))\n",
    "    results[\"single_step_assignment_error_std\"].append(np.std(single_step_incorrect_fracs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae0dcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
